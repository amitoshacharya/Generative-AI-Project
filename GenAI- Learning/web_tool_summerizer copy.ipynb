{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Page Summerizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\\\n",
    "#     langchain_community==0.0.20\\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_url = \"https://raw.githubusercontent.com/iamnaofil/E-commerce-Sales-Analysis/main/Sales%20Data%20Analysis.csv\"\n",
    "# dataset_url = \"https://domo-support.domo.com/s/article/360043931814?language=en_US\"\n",
    "url = \"https://www.chittorgarh.com/report/ipo-performance-report-listing-current-gain/125/all/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_loader = WebBaseLoader(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To bypass SSL verification errors during fetching, you can set the \"verify\" option:\n",
    "\n",
    "## Uncomment to bypass SSL verfication\n",
    "# web_loader.requests_kwargs = {'verify':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = web_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_ENDPOINT = os.getenv(\"OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    model=OPENAI_EMBEDDING_MODEL, \n",
    "    api_key=OPENAI_API_KEY, \n",
    "    azure_endpoint=OPENAI_ENDPOINT, \n",
    "    disallowed_special=(),\n",
    "    )\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HTTPS_PROXY\"]=\"blrproxy.ad.infosys.com:443\"\n",
    "os.environ[\"HTTP_PROXY\"]=\"blrproxy.ad.infosys.com:443\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create db\n",
    "# vector_store = FAISS.from_documents(split_docs, embedding_model)\n",
    "# vector_store.save_local(\"./vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"./vector_store\", embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_retriver = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\":10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_ENDPOINT = os.getenv(\"OPENAI_ENDPOINT\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\")\n",
    "OPENAI_DEPLOYMENT = os.getenv(\"OPENAI_DEPLOYMENT\")\n",
    "OPENAI_API_VERSION = os.getenv(\"OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "        temperature=0,\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=OPENAI_ENDPOINT,\n",
    "        azure_deployment=OPENAI_DEPLOYMENT,\n",
    "        model=OPENAI_MODEL,          \n",
    "        # model_kwargs= { \"top_p\": 1}\n",
    "    )\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Type as \"Stuff\" i.e; Stuff Document\n",
    "\n",
    "stuff_prompt_template = \"\"\"\n",
    "Return the your answer of the following question using the given context.\n",
    "\n",
    "context : {context}\n",
    "question : {question}\n",
    "answer : \n",
    "\"\"\"\n",
    "\n",
    "stuff_prompt = PromptTemplate(template = stuff_prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Type as \"map_reduce\" i.e; Map Reduce Document\n",
    "\n",
    "### 1. Map Reduce Prompt: which will be applied on each batch of the document parallelly.\n",
    "question_prompt_template = \"\"\"\n",
    "Return the answer to the question using the context of information provided below.:\n",
    "\n",
    "text: {context}\n",
    "question : {question}\n",
    "Answer :\n",
    "\"\"\"\n",
    "question_prompt = PromptTemplate(template = question_prompt_template, input_variables=[\"context\"])\n",
    "\n",
    "### 2. Combine Prompt: which will be applied on map reduced results to \n",
    "combine_prompt_template = \"\"\"\n",
    "Generate a summary of the following context.\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "combine_prompt = PromptTemplate(template = combine_prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain_type = \"map_reduce\"\n",
    "\n",
    "if my_chain_type == \"stuff\":\n",
    "    my_chain_type_kwargs = {\n",
    "        'prompt': stuff_prompt\n",
    "    }\n",
    "\n",
    "elif my_chain_type == \"map_reduce\":\n",
    "    my_chain_type_kwargs = {\n",
    "        # \"map_reduce_prompt\": map_reduce_prompt,\n",
    "        \"question_prompt\": question_prompt,\n",
    "        \"combine_prompt\": combine_prompt,\n",
    "        \"combine_document_variable_name\": \"context\",\n",
    "        \"verbose\" : True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm= model,\n",
    "    retriever= qa_retriver,\n",
    "    return_source_documents = True,\n",
    "    verbose = True,\n",
    "    chain_type= my_chain_type,\n",
    "    chain_type_kwargs= my_chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which IPO is performing better in the market?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke({'query':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
