"""
This function uses LLM to process user query and generate responses.
"""

import sys
from config import config
from langchain_community.chat_models import ChatOpenAI, AzureChatOpenAI
from langchain.chains import LLMChain
from utils import chat_response_template

def chat_bot(env:str= 'local'):
    """
    This function creates a llm model with the required params.
    """
    if env == "local":
        bot_llm = ChatOpenAI(
        model = 'gpt-3.5-turbo',
        openai_api_key = config['OPENAI_API_KEY'],
        temperature = 0.6,
        max_tokens = 150
        )
    
    elif env == "azure":
        bot_llm = AzureChatOpenAI(
                        deployment_name=config["AZURE_OPENAI_API_DEPLOYMENT"],
                        openai_api_key =config["AZURE_OPENAI_API_KEY"],
                        openai_api_type="azure",
                        openai_api_base=config["AZURE_OPENAI_API_ENDPOINT"],      
                        openai_api_version=config["AZURE_OPENAI_API_VERSION"],
                        request_timeout=30.00,
                        max_retries=3,
                        temperature=0.25,
                        model_kwargs={"top_p":0.25}
                    )
    
    else:
        print(f"{env} is not a valid environment. Please check and verify.")
        bot_llm = None
        sys.exit()

    return bot_llm

def chat_bot_response(chat_history:list=[], name:str=None, user_query:str= None, env:str= 'local'):
    """
    This function creates a chain using llm model and prompt template. 

    Args:
        user_query: str
    Returns:
        response: str, a query response generated by llm using designed prompt.
    """
    bot_llm = chat_bot(env)
    response_prompt = chat_response_template()
    response_chain = LLMChain(llm = bot_llm, prompt = response_prompt)
    if name is None:
        assistant_name = "Randchod Das"
    else:
        assistant_name = name

    response = response_chain.run({'name':assistant_name, 'query': user_query})
    return response