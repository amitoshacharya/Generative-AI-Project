(4.1) Search Service
Python Folder: SearchService
Decription: This contain LLM model to process user query and generate response to Fast API.
Execution cmd: "from SearchService import chat_bot_response"

(4.1.1) chat_bot_response
Type: Function
Description: This function creates a chain using llm model and prompt template.
--
Args:
    user_query: str
--
Returns:
    response: str, a query response generated by llm using designed prompt.


(4.1.1) chat_bot(env:str= 'local')
Type: Function
Description: This function creates a llm model with the required params.
--
Arg:
    env (str) = 'local' or 'azure'
--

(4.1.1.1) if env == "local"
Type: Condition
Description: if environment parameter is set as local, then use ChatOpenAI
Execution cmd: "from langchain_community.chat_models import ChatOpenAI"
--
model = 'gpt-3.5-turbo'
openai_api_key = config['OPENAI_API_KEY']
temperature = 0.6
max_tokens = 150

(4.1.1.2) if env == "azure"
Type: Condition
Description: if environment parameter is set as local, then use AzureChatOpenAI
Execution cmd: "from langchain_community.chat_models import AzureChatOpenAI"
--
deployment_name=config["AZURE_OPENAI_API_DEPLOYMENT"],
openai_api_key =config["AZURE_OPENAI_API_KEY"],
openai_api_type="azure",
openai_api_base=config["AZURE_OPENAI_API_ENDPOINT"],      
openai_api_version=config["AZURE_OPENAI_API_VERSION"],
request_timeout=30.00,
max_retries=3,
temperature=0.25,
model_kwargs={"top_p":0.25}

(4.1.2) chat_response_template
Python File: utils/prompt_template.py
Description: llm prompt or instruction to llm
--
- name (List[string]): random names of Assistant
+ query (string): user query

